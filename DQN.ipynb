{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNqFcxvpRQXZ"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRXuLcacQY9k",
        "outputId": "c4996ee5-1043-4d4a-a19e-2e894cba18f7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "!pip install stable_baselines3\n",
        "import copy\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "import stable_baselines3\n",
        "import math\n",
        "import random\n",
        "import numpy as np \n",
        "import time\n",
        "import os\n",
        "# Changes depending on Environment: Only MsPacman was tested (DQN wasn't used after this)\n",
        "NUM_ACTIONS = 9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stable_baselines3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/d3/6ae6e774ac6cf8f5eeca1c30b9125231db901b75f72da7d81e939f293f69/stable_baselines3-1.0-py3-none-any.whl (152kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 20.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 17.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 30kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 40kB 13.9MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 51kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 61kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 71kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 81kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 92kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 102kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 112kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 122kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 133kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 143kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.1.5)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (0.17.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable_baselines3) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable_baselines3) (2018.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->stable_baselines3) (3.7.4.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable_baselines3) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable_baselines3) (1.5.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->stable_baselines3) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable_baselines3) (0.16.0)\n",
            "Installing collected packages: stable-baselines3\n",
            "Successfully installed stable-baselines3-1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxPzjUc9Rhbs"
      },
      "source": [
        "### Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzjbgSNYRenT"
      },
      "source": [
        "### Final network architecture\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, in_channels=4, n_actions= NUM_ACTIONS):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.fc4 = nn.Linear(7 * 7 * 64, 512) # hard compute the input size \n",
        "        self.head = nn.Linear(512, n_actions)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.float() / 255 # normalize values to 0 - 1\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
        "        return self.head(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSmf1LReTJDY"
      },
      "source": [
        "### Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO0-OxH_TIIe"
      },
      "source": [
        "# Declare specification for a transition that is stored\n",
        "Transition = namedtuple('Transion', \n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        " Here Transition == Experience. It is comprised of:\n",
        " state: start state - 4 stacked frames \n",
        " action: action chosen in state \n",
        " next_state: the state that the agent ends up in from taking action in state \n",
        " reward: the reward that the agent recieves for taking action in state\n",
        "\"\"\"\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0 # current pointer\n",
        "        \n",
        "    def push(self, *args):\n",
        "        if len(self.memory) < self.capacity: # if buffer not full, append more\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity # if buffer is full then we replace from the beginning in a cicular fashing \n",
        "        \n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size) # sample random batch\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j28rUHNTqtl"
      },
      "source": [
        "### DQN Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q33qVsqAT0B3"
      },
      "source": [
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random() # select a random number to compare against epsilon threshold\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END)* \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY) \n",
        "    steps_done += 1 # increment total steps in env\n",
        "    if sample > eps_threshold:  # if sample is greater then theshold then we select an action according to the policy network \n",
        "        with torch.no_grad():\n",
        "            return policy_net(state.to('cuda')).max(1)[1].view(1,1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(NUM_ACTIONS)]], device=device, dtype=torch.long) # else we select a random action \n",
        "\n",
        "def optimize_model():\n",
        "  # If the buffer is not larger than the batch size, we cannot sample a full batch \n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE) # Get training batch \n",
        "    \"\"\"\n",
        "    zip(*transitions) unzips the transitions into\n",
        "    Transition(*) creates new named tuple\n",
        "    batch.state - tuple of all the states (each state is a tensor)\n",
        "    batch.next_state - tuple of all the next states (each state is a tensor)\n",
        "    batch.reward - tuple of all the rewards (each reward is a float)\n",
        "    batch.action - tuple of all the actions (each action is an int)    \n",
        "    \"\"\"\n",
        "    batch = Transition(*zip(*transitions)) # unzip the zipped transitions \n",
        "    \n",
        "    # extract actions and rewards\n",
        "    actions = tuple((map(lambda a: torch.tensor([[a]], device='cuda'), batch.action))) \n",
        "    rewards = tuple((map(lambda r: torch.tensor([r], device='cuda'), batch.reward))) \n",
        "\n",
        "    # make sure not a terminal state so we don't run into errors\n",
        "    non_final_mask = torch.tensor(\n",
        "        tuple(map(lambda s: s is not None, batch.next_state)),\n",
        "        device=device, dtype=torch.uint8)\n",
        "    \n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                       if s is not None]).to('cuda')\n",
        "    \n",
        "\n",
        "    state_batch = torch.cat(batch.state).to('cuda')\n",
        "    action_batch = torch.cat(actions)\n",
        "    reward_batch = torch.cat(rewards)\n",
        "    \n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch # compute target by taking expected next state value and summing with reward just recieved\n",
        "    \n",
        "    # Computer Huber loss \n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "    \n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() # back prop\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n",
        "\n",
        "\"\"\"\n",
        "Function takes in observation returned from environment, preprocesses and returns 'state'\n",
        "\"\"\"\n",
        "def get_state(obs):\n",
        "    state = np.array(obs)\n",
        "    # print(state.shape)\n",
        "    state = state.transpose((2, 0, 1))\n",
        "    state = torch.from_numpy(state)\n",
        "    return state.unsqueeze(0)\n",
        "\n",
        "def train(env, total_timesteps, render=False):\n",
        "    training_history = []\n",
        "    episode = 0\n",
        "    while steps_done < total_timesteps: # run for total timesteps \n",
        "        episode += 1\n",
        "        obs = env.reset()\n",
        "        state = get_state(obs) # get initial state\n",
        "        total_reward = 0.0\n",
        "        for t in count():\n",
        "            action = select_action(state)\n",
        "\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "            obs, reward, done, info = env.step(action) # step in environment\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            if not done: # check if terminal state\n",
        "                next_state = get_state(obs)\n",
        "            else:\n",
        "                next_state = None\n",
        "\n",
        "            reward = torch.tensor([reward], device=device)\n",
        "\n",
        "            memory.push(state, action.to('cpu'), next_state, reward.to('cpu')) # append an experience\n",
        "            state = next_state\n",
        "\n",
        "            if steps_done > INITIAL_MEMORY: # if we have enough experiences save, optimize the model for one step\n",
        "                optimize_model()\n",
        "\n",
        "                if steps_done % TARGET_UPDATE == 0: # copy over weights every x number of steps from policy to target network\n",
        "                    target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "        training_history.append((total_reward, episode, t, steps_done))\n",
        "        if episode % 20 == 0:\n",
        "                print('Total steps: {} \\t Episode: {}/{} \\t Total reward: {}'.format(steps_done, episode, t, total_reward))\n",
        "    env.close()\n",
        "    return training_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUtjk9RecKNs"
      },
      "source": [
        "### Define Wrapper of Environment\n",
        "\n",
        "This class was taken from StableBaselines3: cited in report\n",
        "I transitioned to using another method later in the project. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us_TN2QyXHnZ"
      },
      "source": [
        "from collections import deque\n",
        "import numpy as np\n",
        "import gym\n",
        "import copy\n",
        "import cv2\n",
        "cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "# Code to wrap environment, taken from StableBaselines3: cited in report\n",
        "\n",
        "def make_env(env, stack_frames=True, episodic_life=True, clip_rewards=False, scale=False):\n",
        "    if episodic_life:\n",
        "        env = EpisodicLifeEnv(env)\n",
        "\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "\n",
        "    env = WarpFrame(env)\n",
        "    if stack_frames:\n",
        "        env = FrameStack(env, 4)\n",
        "    if clip_rewards:\n",
        "        env = ClipRewardEnv(env)\n",
        "    return env\n",
        "\n",
        "class RewardScaler(gym.RewardWrapper):\n",
        "\n",
        "    def reward(self, reward):\n",
        "        return reward * 0.1\n",
        "\n",
        "\n",
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)\n",
        "\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\n",
        "        This object should only be converted to numpy array before being passed to the model.\n",
        "        You'd not believe how complex the previous solution was.\"\"\"\n",
        "        self._frames = frames\n",
        "        self._out = None\n",
        "\n",
        "    def _force(self):\n",
        "        if self._out is None:\n",
        "            self._out = np.concatenate(self._frames, axis=2)\n",
        "            self._frames = None\n",
        "        return self._out\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = self._force()\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._force())\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._force()[i]\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        See Also\n",
        "        --------\n",
        "        baselines.common.atari_wrappers.LazyFrames\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k), dtype=env.observation_space.dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.width = 84\n",
        "        self.height = 84\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
        "            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "        return frame[:, :, None]\n",
        "\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"For environments where the user need to press FIRE for the game to start.\"\"\"\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        super(EpisodicLifeEnv, self).__init__(env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "        self.was_real_reset = False\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert somtimes we stay in lives == 0 condtion for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset()\n",
        "            self.was_real_reset = True\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "            self.was_real_reset = False\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        super(NoopResetEnv, self).__init__(env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset()\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = np.random.randint(1, self.noop_max + 1)\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ = self.env.step(0)\n",
        "            if done:\n",
        "                obs = self.env.reset()\n",
        "        return obs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33qKirb0UGLG"
      },
      "source": [
        "### Train the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IoQ3IkBT5As"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # Path\n",
        "    base_path = './drive/MyDrive/Spring 2021/CS 354/project/dqnOutput/'\n",
        "\n",
        "    # set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # hyperparameters\n",
        "    BATCH_SIZE = 32\n",
        "    GAMMA = 0.99 # discount factor\n",
        "    EPS_START = 1 # how much we explore at start of training\n",
        "    EPS_END = 0.02 # how much we explore as training steps approach infinity\n",
        "    EPS_DECAY = 1000 # how rapidly we decay exploration\n",
        "    TARGET_UPDATE = 10000 # how often we copy over weights from policy network to target network \n",
        "    RENDER = False\n",
        "    LEARNING_RATE = .0005\n",
        "    INITIAL_MEMORY = 1000 # how many experiences need to be stored in memory before we start optimization \n",
        "    MEMORY_SIZE = 50000 # buffer size of memory\n",
        "    TOTAL_TIMESTEPS = 1000 # total timesteps: only ended up training ~ 4.5 before training was ended\n",
        "\n",
        "    # create networks\n",
        "    policy_net = DQN(n_actions=9).to(device) # chooses actions and is the one being backpropogated on\n",
        "    target_net = DQN(n_actions=9).to(device) # computes target value and gets weights from policy every x timesteps \n",
        "    target_net.load_state_dict(policy_net.state_dict()) # copy over weights from policy network\n",
        "\n",
        "    # setup optimizer\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    steps_done = 0 # track how many steps acted in evn\n",
        "\n",
        "    # create environment and wrap it\n",
        "    env = gym.make('MsPacmanNoFrameskip-v4')\n",
        "    env = make_env(env)\n",
        "\n",
        "    # initialize replay memory\n",
        "    memory = ReplayMemory(MEMORY_SIZE)\n",
        "    \n",
        "    # train model\n",
        "    start = time.time() # track total time training took\n",
        "    training_history = train(env, TOTAL_TIMESTEPS) # training history is a tuple: reward per episode, episode number, time steps per episode, total timesteps so far\n",
        "    print('Total time: {}'.format(time.time() - start))\n",
        "    import pickle\n",
        "    os.makedirs(base_path + 'models/', exist_ok = True)\n",
        "    with open(base_path +'training_history.pkl', 'wb') as f:\n",
        "      pickle.dump(training_history, f)\n",
        "    torch.save(policy_net, base_path + \"models/dqn_MsPacman_model\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}