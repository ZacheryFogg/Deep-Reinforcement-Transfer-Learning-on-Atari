{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PolicyGradientAgent.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWSXG6NoX8WU"
      },
      "source": [
        "# Implementing Proximal Policy Optimization: OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1Q32HNOS2Kq"
      },
      "source": [
        "# Imports \n",
        "import numpy as np\n",
        "import tensorflow as tf \n",
        "import gym\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow.keras.losses as kls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_4-xYPxH98"
      },
      "source": [
        "# Create Environment \n",
        "env= gym.make(\"CartPole-v0\")\n",
        "low = env.observation_space.low\n",
        "high = env.observation_space.high"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgLnX0VQYAoZ"
      },
      "source": [
        "### Creating Neual Networks for Actor and Critic\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN3kGA84YBdn"
      },
      "source": [
        "class critic(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.d1 = tf.keras.layers.Dense(128,activation='relu')\n",
        "    self.v = tf.keras.layers.Dense(1, activation = None)\n",
        "\n",
        "  def call(self, input_data):\n",
        "    x = self.d1(input_data)\n",
        "    v = self.v(x)\n",
        "    return v\n",
        "    \n",
        "\n",
        "class actor(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.d1 = tf.keras.layers.Dense(128,activation='relu')\n",
        "    self.a = tf.keras.layers.Dense(2,activation='softmax')\n",
        "\n",
        "  def call(self, input_data):\n",
        "    x = self.d1(input_data)\n",
        "    a = self.a(x)\n",
        "    return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYTN0Zz_xhIi"
      },
      "source": [
        "### Creating Agent Class\n",
        "---\n",
        "Agent has both Actor and Critic networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDcYy83OxfsX"
      },
      "source": [
        "class agent():\n",
        "    def __init__(self, gamma = 0.99):\n",
        "        self.gamma = gamma\n",
        "        # self.a_opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "        # self.c_opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "        self.a_opt = tf.keras.optimizers.Adam(learning_rate=7e-3)\n",
        "        self.c_opt = tf.keras.optimizers.Adam(learning_rate=7e-3)\n",
        "        self.actor = actor()\n",
        "        self.critic = critic()\n",
        "        self.clip_pram = 0.2\n",
        "\n",
        "          \n",
        "    def act(self,state):\n",
        "        prob = self.actor(np.array([state]))\n",
        "        prob = prob.numpy()\n",
        "        dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
        "        action = dist.sample()\n",
        "        return int(action.numpy()[0])\n",
        "  \n",
        "\n",
        "\n",
        "    def actor_loss(self, probs, actions, adv, old_probs, closs):\n",
        "        \n",
        "        probability = probs      \n",
        "        entropy = tf.reduce_mean(tf.math.negative(tf.math.multiply(probability,tf.math.log(probability))))\n",
        "        #print(probability)\n",
        "        #print(entropy)\n",
        "        sur1 = []\n",
        "        sur2 = []\n",
        "        \n",
        "        for pb, t, op in zip(probability, adv, old_probs):\n",
        "                        t =  tf.constant(t)\n",
        "                        op =  tf.constant(op)\n",
        "                        #print(f\"t{t}\")\n",
        "                        #ratio = tf.math.exp(tf.math.log(pb + 1e-10) - tf.math.log(op + 1e-10))\n",
        "                        ratio = tf.math.divide(pb,op)\n",
        "                        #print(f\"ratio{ratio}\")\n",
        "                        s1 = tf.math.multiply(ratio,t)\n",
        "                        #print(f\"s1{s1}\")\n",
        "                        s2 =  tf.math.multiply(tf.clip_by_value(ratio, 1.0 - self.clip_pram, 1.0 + self.clip_pram),t)\n",
        "                        #print(f\"s2{s2}\")\n",
        "                        sur1.append(s1)\n",
        "                        sur2.append(s2)\n",
        "\n",
        "        sr1 = tf.stack(sur1)\n",
        "        sr2 = tf.stack(sur2)\n",
        "        \n",
        "        #closs = tf.reduce_mean(tf.math.square(td))\n",
        "        loss = tf.math.negative(tf.reduce_mean(tf.math.minimum(sr1, sr2)) - closs + 0.001 * entropy)\n",
        "        #print(loss)\n",
        "        return loss\n",
        "\n",
        "    def learn(self, states, actions,  adv , old_probs, discnt_rewards):\n",
        "        discnt_rewards = tf.reshape(discnt_rewards, (len(discnt_rewards),))\n",
        "        adv = tf.reshape(adv, (len(adv),))\n",
        "\n",
        "        old_p = old_probs\n",
        "\n",
        "        old_p = tf.reshape(old_p, (len(old_p),2))\n",
        "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
        "            p = self.actor(states, training=True)\n",
        "            v =  self.critic(states,training=True)\n",
        "            v = tf.reshape(v, (len(v),))\n",
        "            td = tf.math.subtract(discnt_rewards, v)\n",
        "            c_loss = 0.5 * kls.mean_squared_error(discnt_rewards, v)\n",
        "            a_loss = self.actor_loss(p, actions, adv, old_probs, c_loss)\n",
        "            \n",
        "        grads1 = tape1.gradient(a_loss, self.actor.trainable_variables)\n",
        "        grads2 = tape2.gradient(c_loss, self.critic.trainable_variables)\n",
        "        self.a_opt.apply_gradients(zip(grads1, self.actor.trainable_variables))\n",
        "        self.c_opt.apply_gradients(zip(grads2, self.critic.trainable_variables))\n",
        "        return a_loss, c_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghf6or7_yc8C"
      },
      "source": [
        "### Ability to test agent for one episode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQkspMedyb1h"
      },
      "source": [
        "def test_reward(env):\n",
        "  total_reward = 0\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  while not done:\n",
        "    action = np.argmax(agentoo7.actor(np.array([state])).numpy())\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "\n",
        "  return total_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d9k74Yo2AGF"
      },
      "source": [
        "### Preprocess states, actions, rewards, etc... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GIz2_d02GiD"
      },
      "source": [
        "def preprocess1(states, actions, rewards, done, values, gamma):\n",
        "    g = 0\n",
        "    lmbda = 0.95\n",
        "    returns = []\n",
        "    for i in reversed(range(len(rewards))):\n",
        "       delta = rewards[i] + gamma * values[i + 1] * done[i] - values[i]\n",
        "       g = delta + gamma * lmbda * dones[i] * g\n",
        "       returns.append(g + values[i])\n",
        "\n",
        "    returns.reverse()\n",
        "    adv = np.array(returns, dtype=np.float32) - values[:-1]\n",
        "    adv = (adv - np.mean(adv)) / (np.std(adv) + 1e-10)\n",
        "    states = np.array(states, dtype=np.float32)\n",
        "    actions = np.array(actions, dtype=np.int32)\n",
        "    returns = np.array(returns, dtype=np.float32)\n",
        "    return states, actions, returns, adv    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uToPINDq2JLs"
      },
      "source": [
        "### Train Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBxj7oNm2LqD",
        "outputId": "8f9f42b7-1296-44f4-f7a9-3cdbfcb1c52e"
      },
      "source": [
        "tf.random.set_seed(336699)\n",
        "agentoo7 = agent()\n",
        "steps = 3\n",
        "ep_reward = []\n",
        "total_avgr = []\n",
        "target = False \n",
        "best_reward = 0\n",
        "avg_rewards_list = []\n",
        "\n",
        "\n",
        "for s in range(steps):\n",
        "  if target == True:\n",
        "          break\n",
        "  \n",
        "  done = False\n",
        "  state = env.reset()\n",
        "  all_aloss = []\n",
        "  all_closs = []\n",
        "  rewards = []\n",
        "  states = []\n",
        "  actions = []\n",
        "  probs = []\n",
        "  dones = []\n",
        "  values = []\n",
        "  print(\"new episod\")\n",
        "\n",
        "  for e in range(128):\n",
        "   \n",
        "    action = agentoo7.act(state)\n",
        "    value = agentoo7.critic(np.array([state])).numpy()\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    dones.append(1-done)\n",
        "    rewards.append(reward)\n",
        "    states.append(state)\n",
        "    #actions.append(tf.one_hot(action, 2, dtype=tf.int32).numpy().tolist())\n",
        "    actions.append(action)\n",
        "    prob = agentoo7.actor(np.array([state]))\n",
        "    probs.append(prob[0])\n",
        "    values.append(value[0][0])\n",
        "    state = next_state\n",
        "    if done:\n",
        "      env.reset()\n",
        "  \n",
        "  value = agentoo7.critic(np.array([state])).numpy()\n",
        "  values.append(value[0][0])\n",
        "  np.reshape(probs, (len(probs),2))\n",
        "  probs = np.stack(probs, axis=0)\n",
        "\n",
        "  states, actions,returns, adv  = preprocess1(states, actions, rewards, dones, values, 1)\n",
        "\n",
        "  for epocs in range(10):\n",
        "      al,cl = agentoo7.learn(states, actions, adv, probs, returns)\n",
        "      # print(f\"al{al}\") \n",
        "      # print(f\"cl{cl}\")   \n",
        "\n",
        "  avg_reward = np.mean([test_reward(env) for _ in range(5)])\n",
        "  print(f\"total test reward is {avg_reward}\")\n",
        "  avg_rewards_list.append(avg_reward)\n",
        "  if avg_reward > best_reward:\n",
        "        print('best reward=' + str(avg_reward))\n",
        "        agentoo7.actor.save('model_actor_{}_{}'.format(s, avg_reward), save_format=\"tf\")\n",
        "        agentoo7.critic.save('model_critic_{}_{}'.format(s, avg_reward), save_format=\"tf\")\n",
        "        best_reward = avg_reward\n",
        "  if best_reward == 200:\n",
        "        target = True\n",
        "  env.reset()\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new episod\n",
            "total test reward is 194.4\n",
            "best reward=194.4\n",
            "INFO:tensorflow:Assets written to: model_actor_0_194.4/assets\n",
            "INFO:tensorflow:Assets written to: model_critic_0_194.4/assets\n",
            "new episod\n",
            "total test reward is 173.6\n",
            "new episod\n",
            "total test reward is 74.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31nbixBo3c4t"
      },
      "source": [
        "%load_ext tensorboard\n",
        "# %tensorboard --logdir '/content/drive/MyDrive/Spring 2021/CS 354/project/pacman/ppo2_v5/'\n",
        "# %tensorboard --logdir '/content/drive/MyDrive/Spring 2021/CS 354/project/pacman/ppo2_v5/output/tensorboard/ppo2_v5_run40to80_1/'\n",
        "%tensorboard --logdir '/content/drive/MyDrive/Spring 2021/CS 354/project/pacman0to80_real/run0to50/output/tensorboard/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZAQkHuskLVn"
      },
      "source": [
        ""
      ]
    }
  ]
}
